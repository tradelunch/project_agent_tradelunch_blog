# LLM Setup Guide

This project supports **both local and cloud LLM providers**. Tags and descriptions are **always generated by LLM** (not from frontmatter).

## Quick Start

### Option 1: Local LLM (Free, Ollama)

```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull a model (Qwen3 8B recommended)
ollama pull qwen3:8b

# 3. Start Ollama server
ollama serve

# 4. Set environment variables (optional, defaults are fine)
export LLM_PROVIDER="local"
export OLLAMA_MODEL="qwen3:8b"

# 5. Run your agent
python __tests__/test_improved_agents.py
```

### Option 2: OpenAI (Cloud, Paid)

```bash
# 1. Get API key from https://platform.openai.com/api-keys

# 2. Set environment variables
export LLM_PROVIDER="openai"
export OPENAI_API_KEY="sk-..."
export OPENAI_MODEL="gpt-4o-mini"  # or gpt-4o

# 3. Install dependencies (if not already)
pip install -e .

# 4. Run your agent
python __tests__/test_improved_agents.py
```

### Option 3: Anthropic Claude (Cloud, Paid)

```bash
# 1. Get API key from https://console.anthropic.com/

# 2. Set environment variables
export LLM_PROVIDER="anthropic"
export ANTHROPIC_API_KEY="sk-ant-..."
export ANTHROPIC_MODEL="claude-3-5-sonnet-20241022"

# 3. Install dependencies (if not already)
pip install -e .

# 4. Run your agent
python __tests__/test_improved_agents.py
```

---

## Detailed Configuration

### Environment Variables

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `LLM_PROVIDER` | Provider: `local`, `openai`, `anthropic` | `local` | No |
| `LLM_TEMPERATURE` | Temperature for generation (0.0-1.0) | `0.3` | No |
| `LLM_MAX_TOKENS` | Maximum tokens to generate | `2048` | No |

#### Local (Ollama) Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `OLLAMA_BASE_URL` | Ollama server URL | `http://localhost:11434` |
| `OLLAMA_MODEL` | Model name | `qwen3:8b` |

**Available Ollama models:**
- `qwen3:8b` - Qwen3 8B (recommended, fast)
- `llama3.2:8b` - Llama 3.2 8B
- `mistral:7b` - Mistral 7B
- See all: https://ollama.com/library

#### OpenAI Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key | *(required)* |
| `OPENAI_MODEL` | Model name | `gpt-4o-mini` |

**Available OpenAI models:**
- `gpt-4o-mini` - Recommended (cheap, fast)
- `gpt-4o` - Most capable (expensive)
- `gpt-3.5-turbo` - Cheapest (less capable)

**Pricing (as of 2024):**
- `gpt-4o-mini`: $0.15/1M input tokens, $0.60/1M output
- `gpt-4o`: $2.50/1M input, $10/1M output
- See: https://openai.com/api/pricing/

#### Anthropic Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `ANTHROPIC_API_KEY` | Anthropic API key | *(required)* |
| `ANTHROPIC_MODEL` | Model name | `claude-3-5-sonnet-20241022` |

**Available Anthropic models:**
- `claude-3-5-sonnet-20241022` - Recommended (balanced)
- `claude-3-opus-20240229` - Most capable (expensive)
- `claude-3-haiku-20240307` - Fastest (cheapest)

**Pricing (as of 2024):**
- `claude-3-5-sonnet`: $3/1M input, $15/1M output
- `claude-3-opus`: $15/1M input, $75/1M output
- `claude-3-haiku`: $0.25/1M input, $1.25/1M output
- See: https://www.anthropic.com/pricing#anthropic-api

---

## Usage in Code

### Option A: Auto-configuration (Recommended)

Uses `config.LLM_PROVIDER` environment variable:

```python
from agents import ExtractingAgent

# Auto-creates LLM from environment config
agent = ExtractingAgent()  # Uses LLM_PROVIDER

result = await agent.run(task)
```

### Option B: Explicit provider

```python
from llm_factory import create_llm
from agents import ExtractingAgent

# Create specific LLM
llm = create_llm(provider="openai")  # or "local", "anthropic"

# Use with agent
agent = ExtractingAgent(llm=llm)

result = await agent.run(task)
```

### Option C: Disable LLM

```python
from agents import ExtractingAgent

# Disable LLM (will use empty tags/description)
agent = ExtractingAgent(enable_llm=False)

result = await agent.run(task)
```

---

## Testing LLM Providers

Run the test suite to verify your LLM configuration:

```bash
python __tests__/test_llm_providers.py
```

This will:
1. ✅ Test local Ollama connection
2. ✅ Test OpenAI API (if key set)
3. ✅ Test Anthropic API (if key set)
4. ✅ Test ExtractingAgent with LLM

---

## LLM Behavior

### Tags Generation

**Always generated by LLM** (frontmatter tags are ignored):

```markdown
---
tags: [old, frontmatter, tags]  # IGNORED
---
```

LLM will generate 5-7 relevant tags based on content analysis.

### Description Generation

**Always generated by LLM** (frontmatter desc is ignored):

```markdown
---
desc: Old frontmatter description  # IGNORED
---
```

LLM will generate a 3-sentence summary:
1. What is this article about?
2. What will readers learn?
3. Key takeaway or benefit

### Fallback Behavior

If LLM is disabled or fails:
- Tags: `[]` (empty list)
- Description: `""` (empty string)

---

## Troubleshooting

### Local (Ollama)

**Problem:** `Connection refused`

```bash
# Solution: Start Ollama server
ollama serve
```

**Problem:** `Model not found`

```bash
# Solution: Pull the model
ollama pull qwen3:8b
```

**Problem:** Model is slow

```bash
# Solution: Use smaller model or add more RAM
ollama pull qwen3:3b  # Smaller, faster
```

### OpenAI

**Problem:** `Invalid API key`

```bash
# Solution: Check your API key
echo $OPENAI_API_KEY  # Should show sk-...

# Get new key from https://platform.openai.com/api-keys
export OPENAI_API_KEY="sk-..."
```

**Problem:** `Rate limit exceeded`

- **Solution:** Upgrade your OpenAI plan or reduce usage
- See: https://platform.openai.com/account/rate-limits

**Problem:** Too expensive

```bash
# Solution: Use cheaper model
export OPENAI_MODEL="gpt-4o-mini"  # or gpt-3.5-turbo
```

### Anthropic

**Problem:** `Invalid API key`

```bash
# Solution: Check your API key
echo $ANTHROPIC_API_KEY  # Should show sk-ant-...

# Get new key from https://console.anthropic.com/
export ANTHROPIC_API_KEY="sk-ant-..."
```

**Problem:** `Rate limit exceeded`

- **Solution:** Contact Anthropic support or reduce usage
- See: https://console.anthropic.com/

**Problem:** Too expensive

```bash
# Solution: Use cheaper model
export ANTHROPIC_MODEL="claude-3-haiku-20240307"
```

---

## Cost Estimation

For a typical blog post (1000 words):

| Provider | Model | Input Tokens | Output Tokens | Cost per Post | Notes |
|----------|-------|--------------|---------------|---------------|-------|
| **Local** | qwen3:8b | ~1500 | ~100 | **FREE** | Runs on your machine |
| OpenAI | gpt-4o-mini | ~1500 | ~100 | $0.0003 | Cheapest cloud option |
| OpenAI | gpt-4o | ~1500 | ~100 | $0.005 | Best quality |
| Anthropic | claude-3-haiku | ~1500 | ~100 | $0.0005 | Fast and cheap |
| Anthropic | claude-3-5-sonnet | ~1500 | ~100 | $0.006 | Balanced |

**Recommendation:**
- **Development/Testing:** Use local Ollama (free)
- **Production (budget):** OpenAI `gpt-4o-mini` or Anthropic `claude-3-haiku`
- **Production (quality):** Anthropic `claude-3-5-sonnet`

---

## Environment File (.env)

Create a `.env` file for easy configuration:

```bash
# .env
LLM_PROVIDER=local  # or openai, anthropic

# Local (Ollama)
OLLAMA_MODEL=qwen3:8b

# OpenAI (uncomment to use)
#OPENAI_API_KEY=sk-...
#OPENAI_MODEL=gpt-4o-mini

# Anthropic (uncomment to use)
#ANTHROPIC_API_KEY=sk-ant-...
#ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Common settings
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2048
```

Then load it:

```bash
# Load .env file
export $(cat .env | xargs)

# Or use python-dotenv
pip install python-dotenv
```

---

## Best Practices

1. **Development:** Use local Ollama for free, unlimited testing
2. **Production:** Use cloud APIs for better quality and reliability
3. **API Keys:** Never commit API keys to git. Use environment variables
4. **Rate Limits:** Monitor usage and set up alerts
5. **Cost Control:** Start with cheaper models, upgrade if needed
6. **Fallback:** Always handle LLM failures gracefully

---

## Next Steps

1. ✅ Choose your LLM provider (local or cloud)
2. ✅ Set environment variables
3. ✅ Run `python __tests__/test_llm_providers.py` to verify
4. ✅ Run `python __tests__/test_improved_agents.py` to test full workflow
5. ✅ Start processing blog posts!

For more help, see:
- [README.md](../README.md) - Project overview
- [config.py](../config.py) - Configuration entry point (re-exports from `configs/` modules)
- [configs/llm.py](../configs/llm.py) - LLM-specific configuration
- [llm_factory.py](../llm_factory.py) - LLM implementation details
